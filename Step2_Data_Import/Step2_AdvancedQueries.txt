/////////////////
1 - Query
search=*&$select=description&$filter=duration eq "5"

{
  "@odata.context": "https://cognitive-search-example.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 0.2876821,
      "Key": "company-moodle578a3319-aa7c-4d2f-b6a4-39e9638b0a85",
      "description": "For administrators, this course will teach you how our CI/CD pipelines work from an operations perspective",
      "duration": "5",
      "level": "intermediate",
      "product": "jenkins",
      "rating_average": 4.9,
      "rating_count": 56,
      "role": "admin",
      "source": "Company Moodle",
      "title": "DevOps for Ops",
      "url": "https://www.example.com/course5",
      "keyphrases": [
        "company",
        "moodle"
      ]
    }
  ]
}

/////////////////
2 - Query 
search=AI&$select=duration,title&$filter=rating_average lt 5

{
  "@odata.context": "https://cognitive-search-example.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 3.4277406,
      "Key": "ms-learn0197c8c6-dfc4-450b-9fa7-3f610977cc79",
      "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.",
      "duration": "55",
      "level": "beginner",
      "product": "ai-builder",
      "rating_average": 4.61,
      "rating_count": 197,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Text recognition",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-text-recognition/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 2.384315,
      "Key": "ms-learn002f4436-7360-4daa-a21d-f9dcd3518589",
      "description": "Enable business users with key AI use cases",
      "duration": "34",
      "level": "beginner",
      "product": "power-platform",
      "rating_average": 4.75,
      "rating_count": 758,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Enable business users with key AI use cases",
      "url": "https://docs.microsoft.com/en-us/learn/modules/enable-business-users-with-key-ai-uses-cases/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 0.7847684,
      "Key": "ms-learn00baaa75-89fe-4f86-805f-f08336e6af48",
      "description": "Explore the strategic components, use cases, and special factors of an enterprise AI strategy that creates real business value, with INSEAD and Microsoft.",
      "duration": "70",
      "level": "intermediate",
      "product": "m365",
      "rating_average": 4.71,
      "rating_count": 2779,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Define an AI strategy to create business value",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-strategy-to-create-business-value/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 0.5377023,
      "Key": "ms-learn01731c10-20bb-41e7-ba21-8528669dcdc3",
      "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "duration": "18",
      "level": "advanced",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 137,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 0.4210357,
      "Key": "company-moodle6b5d3f55-eb02-499d-9775-2e0e25659e07",
      "description": "Learn our company's Principles for the Responsible Use of AI",
      "duration": "1",
      "level": "intermediate",
      "product": "NA",
      "rating_average": 4.3,
      "rating_count": 24,
      "role": "architect",
      "source": "Company Moodle",
      "title": "Ethics in AI",
      "url": "https://www.example.com/course12",
      "keyphrases": [
        "company",
        "moodle"
      ]
    }
  ]
}
/////////////////
3 - Query
{
    "search": "search=AI&$select=duration,title",
    "filter": "metadata_author eq 'Alina Köchling ' "
  }


{
    "@odata.context": "https://cognitive-search-example.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
    "value": [
      {
        "@search.score": 5.862564,
        "content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&amp;domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et al. 2019).\n\nWe considered scholarly articles from a high-quality source of evidence (peer-\n\nreviewed and published) journals in English and excluded book reviews, comments,\n\nand editorial notes. Moreover, we searched for unpublished articles in conference\n\nproceedings from renowned conferences, such as AOM, EURAM, ACM, and IEEE,\n\nand contacted the authors to prevent publication bias and to gain further valuable\n\n1 We thank the anonymous reviewer for this valuable recommendation.\n\n804 Business Research (2020) 13:795–848\n\n123\n\n\n\ninsights (Siddaway et al. 2019; Lipsey and Wilson 2001; Ferguson and Brannick\n\n2012). In April 2020, this search approach resulted in 3207 articles.\n\n3.2 Screening, eligibility process, and inclusion process\n\nFollowing this initial identification, we manually screened each article (title and\n\nabstract) to evaluate whether its content was fundamental relevant to impact bias,\n\ndiscrimination, or fairness of algorithmic decision-making in HRM, especially in\n\nrecruitment, selection, development, and training in particular. The process of\n\nTable 2 Overview of search terms, databases, and results\n\nSearch string Database Resultsa\n\nTITLE: (‘‘algorithm* OR algorithmic model* OR data-algorithm*OR algorithmic decision-making OR\n\nalgorithmic decision* OR artificial intelligence OR facial expression tool* OR facial expression\n\nprocessing* OR language processing* OR natural language processing* OR recommender system* OR\n\nsearch engine* OR data*OR data set*’’)\n\nTOPIC: (‘‘discrimination* OR discriminat* OR classification* OR ‘‘classification problem*’’ OR\n\n‘‘classification scheme*’’ OR ‘‘algorithmic discrimination*’’ OR ‘‘algorithmic bias discrimination*’’\n\nOR ‘‘preventing discrimination*’’ OR anti-discrimination* OR non-discrimination* OR gender, age,\n\nsex, sexism, origin OR ‘‘difference* among demographic group*’’ OR ethic* OR ‘‘ethical\n\nimplication*’’ OR ‘‘data mining discrimination*’’ OR ‘‘unfair treatment*’’ OR fair* OR unfair* OR\n\n‘‘perceived fairness’’ OR ‘‘algorithmic fairness’’ OR ‘‘fairness word*’’ OR ‘‘fairness speech*’’ OR\n\n‘‘fairness recommendation*’’ OR equal* OR equit* OR inequal* OR ‘‘equal opportunit*’’ OR\n\ntransparen* OR legal* OR right* OR truth OR impartial* OR correct*OR evaluat* OR judgement* OR\n\n‘‘algorithmic judgement*’’ OR ‘‘human judgement*’’ OR ‘‘mechanical judgement*’’ OR rank* OR\n\nrate* OR measure* OR valuation* OR bias* OR ‘‘algorithmic bias*’’ OR ‘‘national bias*’’ OR gender-\n\nbias* OR ‘‘decision-making bias*’’ OR ‘‘human bias* OR ‘‘technical bias*’’ OR ‘‘implicit bias* in\n\nalgorithm*’’ OR ‘‘dealing with bias*’’ OR ‘‘pattern distortion*’’ OR pre-justice* OR tendenc* OR\n\nprone*OR justiceb OR adverse impactb) AND TOPIC: (‘‘Human Resource*’’ OR ‘‘Human Resource\n\nManagement’’ OR Management OR ‘‘applicant selection*’’ OR ‘‘employee selection*’’ OR ‘‘algorithm-\n\nbased HR decision-making’’ OR ‘‘recruitment process* OR ‘‘application process*’’ OR ‘‘selection\n\nprocess*’’ OR recruitment* OR online-recruitment* OR ‘‘personnel decision*’’, OR ‘‘personnel\n\nselection*’’ OR ‘‘people analytic*’’ OR ‘‘HR analytic*’’ OR ‘‘job advertisement*’’ OR ‘‘online\n\npersonalization*’’)\n\nDOCUMENT TYPES = (ARTICLE)\n\nAND\n\nLANGUAGES = (ENGLISH)\n\nSSCI\n\npsychology, psychology experimental, psychology\n\nmultidisciplinary science, ethics, law, psychology\n\napplied, operations research management science,\n\ncomputer science artificial intelligence, computer\n\nscience interdisciplinary applications, computer\n\nscience information systems, management,\n\nbusiness, behavioral science, social sciences\n\ninterdisciplinary, sociology, social issues,\n\nhumanities interdisciplinary\n\n2892\n\narticles\n\nScholarly (Peer Reviewed) Journals,\n\nAcademic Journal, Article English\n\nEBSCO Business Source Premier 244\n\narticles\n\naResults show the gross hits per search string and database for scholarly articles\nbRobustness check\n\nBusiness Research (2020) 13:795–848 805\n\n123\n\n\n\nrelevance screening resulted in 102 articles that were deemed to be substantially\n\nrelevant.\n\nSecond, we conducted the eligibility stage by reading the full text and shifting\n\nfrom sensitivity to specificity. Studies eligible for our review (1) had to be\n\nconsistent with our definition of algorithmic decision-making as well as with our\n\ndefinitions of fairness, bias, or discrimination (2), and the content had to refer to\n\nHRM (3). The list of studies that we excluded at the eligibility stage is available\n\nupon request. The two authors independently checked each paper to increase the\n\nreliability of the research results. We applied this structured approach to ensure a\n\nhigh level of objectivity.\n\nAfterward, the actual review started, and we synthesized and assessed our\n\nfindings. We analyzed the material abductively following a set of predefined\n\ncategories without, however, relying on preexisting codes to extract all relevant\n\ninformation. Analytic categories were, for example, ‘‘research design,’’ ‘‘field of the\n\njournal,’’ ‘‘research geography,’’ or ‘‘year of publication,’’ and ‘‘key findings.’’\n\nAgain, the authors filled these categories with their inductively generated codes.\n\nOur systematic review used the Preferred Reporting Items for Systematic\n\nReviews (PRISMA) recommendations, including assessment of research content as\n\nwell as a detailed report of the number of records identified through the search and\n\nthe number of studies included and excluded in the review. Figure 1 presents a\n\nPRISMA flow diagram to provide a succinct summary of the process (Siddaway\n\net al. 2019; Moher et al. 2009).\n\n3.3 Robustness check\n\nWe implemented a robustness check to offer a reliable and coherent picture of the\n\ndiscrimination potential and fairness when using algorithmic decision-making in\n\nHR recruitment and HR development. With the robustness check, we want to ensure\n\nthat all relevant articles were included in the literature review. We conducted the\n\nrobustness check 3 months after the actual search process with two additional\n\nkeywords, namely: ‘‘justice’’ and ‘‘adverse impact’’ (see Table 2). The search in the\n\ndatabase SSCI resulted in 632 articles and the EBSCO search in 690 articles. We\n\nmanually screened each article (title and abstract) to assess whether the content was\n\nessentially relevant to bias, discrimination, or the fairness of algorithmic decision-\n\nmaking in HRM, especially recruitment, selection, training, and development. The\n\nmajority of articles dealt with the fairness of algorithmic decision-making, but had\n\nno reference to HR. After manually screening each article, the process of relevance\n\nscreening resulted in eight articles for the eligibility stage. We found that no further\n\narticles can be included in the literature review by reading the full text. Since out of\n\nthese eight articles, three articles were already included in the literature review (Lee\n\n2018; Tambe et al. 2019; Yarger et al. 2019), two articles were excluded in the\n\neligibility stage of the initial search process (Hoffmann 2019; Sumser 2017) (no\n\nreference to HRM and comment), and the remaining three articles neither discussed\n\nfairness nor the HR recruitment and/or HR development context (Varghese et al.\n\n1988; Horton 2017; Gil-Lafuente and Oh 2012). The robustness check verified that\n\nthe literature review offers a reliable and transparent picture of the current literature\n\n806 Business Research (2020) 13:795–848\n\n123\n\n\n\nregarding the discrimination potential and fairness when using algorithmic decision-\n\nmaking in HR recruitment and HR development.\n\n3.4 Limitations of the research process\n\nThis approach is not without limitations. First, the reliance on two databases might\n\nbe regarded as a limitation; however, the approach of selecting two broad and\n\ncommon databases contributed to the validity and replicability of our findings due to\n\nthe extensive coverage of high-impact, peer-reviewed journals in these databases\n\n(Podsakoff et al. 2005). Second, our review focused on two essential HR functions\n\nthat have severe consequences for individuals and society concerning ethics, namely\n\nHR recruitment and HR development. We did not consider other areas of HRM,\n\nsince the focus of other HR functions is mainly the automation process (e.g., pay or\n\nanother administrative task). Thus, the situation is different in HR recruitment and\n\nHR development, because societal decisions are made, which have crucial\n\nconsequences for the individual applicants and employees, such as job offer or\n\npromotion opportunities. Especially when it comes to decisions about individuals\n\nRecords identified through \ndatabase searching\n\n(n = 3,136)\nSc\nre\nen\nin\ng\n\nIn\ncl\nud\n\ned\nEl\nig\nib\nili\nty\n\nId\nen\ntif\nic\nat\nio\nn\n\nAdditional records identified \nthrough other sources\n\n(n = 71)\n\nRecords after duplicates removed\n(n = 3,204)\n\nRecords screened\n(n = 3,204)\n\nRecords excludeda\n\n(n = 3,102)\n\nFull-text articles \nassessed for eligibility\n\n(n = 102)\n\nFull-text articles \nexcluded, with reasonsb\n\n(n = 66)\n\nStudies included in \nliterature review\n\n(n = 36)\n\nFig. 1 PRISMA flow diagram illustrating the process. aTopic did not fit, mostly no HR and/or fairness,\nno obvious discrimination context, bMostly no HR and/or fairness, no discrimination context after\nreading the full text or not meeting the inclusion criteria\n\nBusiness Research (2020) 13:795–848 807\n\n123\n\n\n\nand their potential, objective and perceived fairness is paramount (Ötting and Maier\n\n2018; Lee 2018).\n\nMoreover, only articles written in the English-language were part of the literature\n\nreview. Even though this procedure is accepted practice and there is some evidence\n\nthat including only English articles does not bias the results, it should be noted that\n\nnon-English articles were not included because English is the dominant language in\n\nresearch (Morrison et al. 2012).\n\n4 Descriptive results\n\nThe following section shows the current research landscape. We summarize the\n\nmain characteristics of the identified articles in Table 3 and present the main\n\nfindings in Table 4. This table reports the name of authors, year of publication, the\n\nmain focus of the study (i.e., focus on bias, discrimination, fairness, or perceived\n\nfairness), applied method, the field of research, algorithmic decision-making\n\nsystem, HR context (i.e., recruitment- distinguished between recruitment and\n\nselection- or development), and the key findings. We analyze the main focus and the\n\nkey findings of the studies in the following sections. The table is sorted by the focus\n\nof the article and whether it is on bias as a trigger for unfairness and discrimination\n\nor specifically on fairness and discrimination.\n\nFigure 2 illustrates the distribution of publications over time and the research\n\nmethods used. The first identified article in our sample of literature was published in\n\n2014. From 2014 to 2016, only a few articles are published per year. From 2017,\n\ninterest in algorithmic decision-making and discrimination increased notably. As\n\nshown in Fig. 2, there was enormous interest in the topic in 2019.\n\nFrom a methodological perspective, another noteworthy result of this systematic\n\nreview is the predominance of non-empirical evidence, as Table 3 and Fig. 2 show\n\nthat the large majority of articles are non-empirical (i.e., conceptual paper, reviews,\n\nand case studies). A reason for this is that scientific investigation of discrimination\n\nby algorithmic decision-making represents a relatively new topic. However, the\n\nnumber of quantitative papers increased from 2018. Most of the studies focused on\n\nbias, discrimination, and objective fairness, while 12 studies examined perceived\n\nfairness perceptions of applicants and employees (see Table 1). Furthermore, the\n\nmajority of studies are located in the area of recruitment and selection, whereby\n\nthese studies mostly focus on selection. Twelve studies are located in the area of HR\n\ndevelopment. The majority of studies provided either no geographical specification\n\nor were conducted in the USA (see Table 3).\n\nThirteen articles originate from management, and fourteen articles originate\n\nfrom computer science, four articles originate from law, two from psychology, two\n\nfrom information systems, and one from the behavioral sciences. This distribution\n\nillustrates that the field does not have a core in business and management research\n\nand is rather interdisciplinary. Nevertheless, the majority of articles originating from\n\nmanagement were published in high-ranked journals, such as Journal of Business\nEthics, Human Resource Management Review, Management Science, Academy of\nManagement Annals, and Journal of Management. The majority of these studies\n\n808 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nO\nv\ner\nv\nie\nw\n\no\nf\nst\nu\nd\nie\ns\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nN\nai\nm\n\net\nal\n.\n(2\n0\n1\n6\n)\n\nT\nh\ne\nau\nto\nm\nat\ned\n\nan\nal\ny\nsi\ns\no\nf\nfa\nci\nal\n\nex\np\nre\nss\nio\nn\ns,\n\nla\nn\ng\nu\nag\ne,\n\nan\nd\n\np\nro\nso\nd\nic\n\nin\nfo\nrm\n\nat\nio\nn\no\nf\n\nin\nte\nrv\nie\nw\nee\ns\nin\n\na\n\njo\nb\nin\nte\nrv\nie\nw\n\nB\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nan\nti\nta\nti\nv\ne;\n\nan\nal\ny\nsi\ns\no\nf\n\nin\nte\nrv\nie\nw\ns\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nN\nL\nP\n,\nF\nE\nP\n\nS\nel\nec\nti\no\nn\n\nR\nec\no\nm\nm\nen\nd\ns\nto\n\nsp\nea\nk\n\nm\no\nre\n\nfl\nu\nen\ntl\ny\n,\nu\nse\n\nle\nss\n\nfi\nll\ner\n\nw\no\nrd\ns,\nan\nd\nsm\n\nil\ne\n\nm\no\nre\n\nS\nh\no\nw\ns\nth\nat\n\nth\ne\nst\nu\nd\nen\nts\n\nw\nh\no\nw\ner\ne\nra\nte\nd\nh\nig\nh\nly\n\nw\nh\nil\ne\nan\nsw\n\ner\nin\ng\nth\ne\n\nfi\nrs\nt\nin\nte\nrv\nie\nw\n\nq\nu\nes\nti\no\nn\n\nw\ner\ne\nal\nso\n\nra\nte\nd\nh\nig\nh\nly\n\no\nv\ner\nal\nl\n(i\n.e\n.,\nfi\nrs\nt\n\nim\np\nre\nss\nio\nn\nm\nat\nte\nrs\n)\n\nU\nS\nA\n\nC\nh\nen\ng\nan\nd\n\nH\nac\nk\net\nt\n(2\n0\n1\n9\n)\n\nA\ncr\nit\nic\nal\n\nre\nv\nie\nw\n\no\nf\n\nal\ng\no\nri\nth\nm\ns\nin\n\nH\nR\nM\n\nB\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nsi\nn\ng\nle\n\nca\nse\n\nst\nu\nd\ny\n;\nre\nv\nie\nw\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nO\nrg\nan\niz\nat\nio\nn\ns\nh\nav\ne\nto\n\nin\ncr\nea\nse\n\nth\ne\np\ner\nce\niv\ned\n\nau\nth\nen\nti\nci\nty\n\no\nf\n\nal\ng\no\nri\nth\nm\ns\n\nN\nee\nd\nto\n\nev\nal\nu\nat\ne\n\nal\ng\no\nri\nth\nm\ns\nfr\no\nm\n\na\n\nre\nse\nar\nch\n\np\ner\nsp\nec\nti\nv\ne\n\nL\nac\nk\nb\net\nw\nee\nn\np\nra\nct\nic\ne\n\nan\nd\nre\nse\nar\nch\n\nN\no\nt sp\nec\nifi\ned\n\nBusiness Research (2020) 13:795–848 809\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nM\nan\nn\nan\nd\nO\n’N\n\nei\nl\n\n(2\n0\n1\n6\n)\n\nE\nx\np\nla\nin\ns\nw\nh\ny\n\nal\ng\no\nri\nth\nm\ns\nar\ne\n\nn\no\nt\nn\neu\ntr\nal\n\nan\nd\n\no\nff\ner\ns\nso\nm\ne\n\nim\np\nli\nca\nti\no\nn\ns\nto\n\nre\nd\nu\nce\n\nth\ne\nri\nsk\n\no\nf\n\nb\nia\nse\ns\no\nf\n\nal\ng\no\nri\nth\nm\nic\n\nd\nec\nis\nio\nn\n-m\n\nak\nin\ng\n\nin\nth\ne\nh\nir\nin\ng\n\np\nro\nce\nss\n\nB\n,\nD\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n,\nh\nir\nin\ng\n\nal\ng\no\nri\nth\nm\ns\n\nS\nel\nec\nti\no\nn\n\nA\nlg\no\nri\nth\nm\ns\nre\nfl\nec\nt\n\nh\nu\nm\nan\n\nb\nia\nse\ns\nan\nd\n\np\nre\nju\nd\nic\nes\n\nth\nat\n\nle\nad\n\nto\n\nm\nac\nh\nin\ne\nle\nar\nn\nin\ng\n\nm\nis\nta\nk\nes\n\nan\nd\n\nm\nis\nin\nte\nrp\nre\nta\nti\no\nn\ns\n\nB\nia\ns\nan\nd\np\nre\nju\nd\nic\ne\n\nca\nn\nn\no\nt\nb\ne\nco\nm\np\nle\nte\nly\n\nel\nim\n\nin\nat\ned\n\nfr\no\nm\n\nh\nir\nin\ng\n\nH\nR\np\nro\nfe\nss\nio\nn\nal\ns\nm\nu\nst\n\nco\nn\nsi\nd\ner\n\nth\ne\n\nco\nn\nse\nq\nu\nen\nce\ns\no\nf\nth\nes\ne\n\nsy\nst\nem\n\ns\nan\nd\nen\nsu\nre\n\nth\ney\n\nal\nw\nay\ns\nre\nfl\nec\nt\nth\ne\n\nb\nes\nt\nh\nu\nm\nan\n\nin\nte\nn\nti\no\nn\ns\n\nU\nS\nA\n\nK\nim\n\n(2\n0\n1\n7\n)\n\nE\nx\nam\n\nin\nes\n\nth\ne\nu\nse\n\no\nf\n\ncl\nas\nsi\nfi\nca\nti\no\nn\n\nsc\nh\nem\n\nes\n/d\nat\na\n\nal\ng\no\nri\nth\nm\ns\nin\n\nte\nrm\n\ns\no\nf\n\np\ner\nso\nn\nn\nel\n\nd\nec\nis\nio\nn\ns\n\nS\nh\no\nw\ns\nli\nm\nit\nat\nio\nn\ns\nin\n\nex\nis\nti\nn\ng\nla\nw\n\nan\nd\n\nim\np\nro\nv\nem\n\nen\nt\n\np\nro\np\no\nsa\nls\n\nB\n,\nD\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nL\naw\n\nG\nen\ner\nal\n,\n\ncl\nas\nsi\nfi\nca\nti\no\nn\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n,\n\ntr\nai\nn\nin\ng\nan\nd\n\nd\nev\nel\no\np\nm\nen\nt\n\nB\nec\nau\nse\n\no\nf\nth\ne\nn\nat\nu\nre\n\no\nf\n\nd\nat\na\nm\nin\nin\ng\n\nte\nch\nn\niq\nu\nes\n,\nem\n\np\nlo\ny\ner\n\nre\nli\nan\nce\n\no\nn\nth\nes\ne\nto\no\nls\n\np\no\nse\ns\nn\no\nv\nel\n\nch\nal\nle\nn\ng\nes\n\nto\n\nw\no\nrk\np\nla\nce\n\neq\nu\nal\nit\ny\n\nU\nS\nA\n\n810 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nR\no\nse\nn\nb\nla\nt\net\n\nal\n.\n\n(2\n0\n1\n6\n)\n\nE\nx\nam\n\nin\nes\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nth\no\nro\nu\ng\nh\n\nev\nal\nu\nat\nio\nn\nin\n\nth\ne\n\nca\nse\n\no\nf\nU\nb\ner\n\nd\nri\nv\ner\ns\n\nB\n,\nD\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nca\nse\n\nst\nu\nd\ny\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nE\nv\nal\nu\nat\nio\nn\n\nsy\nst\nem\n\ns\n\nD\nev\nel\no\np\nm\nen\nt\n\nT\nh\ne\nn\nee\nd\nto\n\nex\ner\nci\nse\n\nq\nu\nal\nit\ny\nco\nn\ntr\no\nl\no\nv\ner\n\na\n\nla\nrg\ne\nd\nis\nag\ng\nre\ng\nat\ned\n\nw\no\nrk\nfo\nrc\ne\nm\nay\n\np\ner\nm\nit\n\nth\ne\nco\nn\nti\nn\nu\ned\n\nu\nse\n\no\nf\n\nra\nti\nn\ng\n\nS\ny\nst\nem\n\ns\nu\nn\nd\ner\n\nex\nis\nti\nn\ng\n\nem\np\nlo\ny\nm\nen\nt\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\nla\nw\n\nN\no\nt sp\nec\nifi\ned\n\nS\nav\nag\ne\nan\nd\nB\nal\nes\n\n(2\n0\n1\n7\n)\n\nE\nx\nam\n\nin\nes\n\nh\no\nw\n\nv\nid\neo\n\ng\nam\n\ne\n\nal\ng\no\nri\nth\nm\ns\nar\ne\n\nin\nco\nrp\no\nra\nte\nd\nin\nto\n\nth\ne\njo\nb\nh\nir\nin\ng\n\np\nro\nce\nss\n\nS\nh\no\nw\ns\nth\ne\nd\neb\nat\ne\n\no\nv\ner\n\nw\nh\net\nh\ner\n\nth\nes\ne\nal\ng\no\nri\nth\nm\ns\n\nd\nis\ncr\nim\n\nin\nat\ne\n\nB\n,\nD\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nL\naw\n\nG\nam\n\nifi\nca\nti\no\nn\n\nS\nel\nec\nti\no\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt\n\nV\nid\neo\n\ng\nam\n\nes\nin\n\nin\nit\nia\nl\n\nh\nir\nin\ng\nst\nag\nes\n\nca\nn\n\np\ner\nm\nit\nn\no\nn\n-\n\nd\nis\ncr\nim\n\nin\nat\no\nry\n\nev\nal\nu\nat\nio\nn\no\nf\nal\nl\nth\ne\n\nca\nn\nd\nid\nat\nes\n\nU\nS\nA\n\nBusiness Research (2020) 13:795–848 811\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nW\nil\nli\nam\n\ns\net\n\nal\n.\n\n(2\n0\n1\n8\n)\n\nE\nx\nam\n\nin\nes\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nth\nro\nu\ng\nh\nth\ne\nu\nse\n\no\nf\n\nal\ng\no\nri\nth\nm\ns\nin\n\nd\nec\nis\nio\nn\n-m\n\nak\nin\ng\n\np\nro\nce\nss\nes\n\nP\nro\np\no\nse\ns\nst\nra\nte\ng\nie\ns\n\nfo\nr\nth\ne\np\nre\nv\nen\nti\no\nn\n\no\nf\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nB\n,\nD\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nm\nu\nlt\nip\nle\n\nca\nse\n\nst\nu\nd\nie\ns\n\nIn\nfo\nrm\n\nat\nio\nn\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nA\nlg\no\nri\nth\nm\nic\n\np\nre\nd\nic\nti\no\nn\n\nca\nn\nin\ncl\nu\nd\ne\nin\nju\nst\nic\nes\n\nT\nh\ne\np\nre\nd\nic\nti\no\nn\ns\nh\nav\ne\nto\n\nb\ne\nch\nec\nk\ned\n\nfo\nr\nb\nia\ns\n\nan\nd\nsh\no\nu\nld\n\nb\ne\n\nco\nrr\nec\nte\nd\nto\n\nav\no\nid\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nU\nS\nA\n\nL\nam\n\nb\nre\nch\nt\nan\nd\n\nT\nu\nck\ner\n\n(2\n0\n1\n9\n)\n\nE\nx\nam\n\nin\nes\n\ng\nen\nd\ner\n\nb\nia\ns\nin\n\nd\nel\niv\ner\ny\no\nf\n\njo\nb\nad\ns\n\nC\no\nn\nd\nu\nct\ns\nfi\nel\nd\nte\nst\n\no\nf\nh\no\nw\n\nan\n\nal\ng\no\nri\nth\nm\n\nd\nel\niv\ner\ned\n\nad\ns\n\np\nro\nm\no\nti\nn\ng\njo\nb\n\no\np\np\no\nrt\nu\nn\nit\nie\ns\nin\n\nth\ne\nsc\nie\nn\nce\n,\n\nte\nch\nn\no\nlo\ng\ny\n,\n\nen\ng\nin\nee\nri\nn\ng\nan\nd\n\nm\nat\nh\nfi\nel\nd\ns\n\nB\n,\nD\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nan\nti\nta\nti\nv\ne;\n\nfi\nel\nd\nte\nst\n\nM\nan\nag\nem\n\nen\nt\n\nR\nec\no\nm\nm\nen\nd\ner\n\nsy\nst\nem\n\ns\n\nR\nec\nru\nit\nm\nen\nt\n\nF\new\n\ner\nw\no\nm\nen\n\nsa\nw\n\nS\nT\nE\nM\n\nad\ns\nth\nan\n\nm\nen\n\nA\nn\nal\ng\no\nri\nth\nm\n\nth\nat\n\nsi\nm\np\nly\n\no\np\nti\nm\niz\nes\n\nco\nst\n-\n\nef\nfe\nct\niv\nen\nes\ns\nin\n\nad\n\nd\nel\niv\ner\ny\nw\nil\nl\nd\nel\niv\ner\n\nad\ns\nth\nat\n\nw\ner\ne\nin\nte\nn\nd\ned\n\nto\nb\ne\ng\nen\nd\ner\n\nn\neu\ntr\nal\n\nin\n\nan\nap\np\nar\nen\ntl\ny\n\nd\nis\ncr\nim\n\nin\nat\no\nry\n\nw\nay\n\n1\n9\n1 co\nu\nn\ntr\nie\ns\n\n812 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nS\naj\nja\nd\nia\nn\ni\net\n\nal\n.\n\n(2\n0\n1\n9\n)\n\nP\nre\nd\nic\nti\no\nn\no\nf\nfu\ntu\nre\n\nw\no\nrk\n\no\nu\ntc\no\nm\nes\n\nsu\nch\n\nas\nv\no\nlu\nn\nta\nry\n\ntu\nrn\no\nv\ner\n,\n\nin\nv\no\nlu\nn\nta\nry\n\ntu\nrn\no\nv\ner\n,\nan\nd\n\nv\nal\nu\ne-\nad\nd\ned\n\nb\nas\ned\n\no\nn\nw\no\nrk\n\nh\nis\nto\nry\n\ncr\nit\ner\nia\n\n(w\no\nrk\n\nex\np\ner\nie\nn\nce\n\nre\nle\nv\nan\nce\n,\nte\nn\nu\nre\n\nh\nis\nto\nry\n,\n\nat\ntr\nib\nu\nti\no\nn\ns\nfo\nr\n\np\nre\nv\nio\nu\ns\n\ntu\nrn\no\nv\ner\n)\n\nB\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nan\nti\nta\nti\nv\ne,\n\nst\nat\nis\nti\nca\nl\n\nte\nst\ns;\n\nm\nac\nh\nin\ne\n\nle\nar\nn\nin\ng\n\nP\nsy\nch\no\nlo\ng\ny\n\nG\nen\ner\nal\n\nS\nel\nec\nti\no\nn\n\nA\nlg\no\nri\nth\nm\nic\nm\net\nh\no\nd\ns\nar\ne\n\no\nft\nen\n\nca\nli\nb\nra\nte\nd\n\nex\ncl\nu\nsi\nv\nel\ny\nto\n\na\n\nsp\nec\nifi\nc\nap\np\nli\nca\nti\no\nn\n\np\no\no\nl\n\nP\nai\nri\nn\ng\nes\nta\nb\nli\nsh\ned\n\nth\neo\nry\n\nle\nad\ns\nto\n\nb\net\nte\nr\n\nre\nsu\nlt\ns\nth\nan\n\nu\nn\niq\nu\ne\n\nw\no\nrd\n\nap\np\nli\nca\nti\no\nn\ns\n\nU\nS\nA\n\nY\nar\ng\ner\n\net\nal\n.\n\n(2\n0\n1\n9\n)\n\nC\nri\nti\nca\nl\nan\nal\ny\nsi\ns\no\nf\n\nta\nle\nn\nt\nac\nq\nu\nis\nit\nio\nn\n\nso\nft\nw\nar\ne\nan\nd\nit\ns\n\np\no\nte\nn\nti\nal\n\nfo\nr\n\nfo\nst\ner\nin\ng\neq\nu\nit\ny\n\nin\nth\ne\nh\nir\nin\ng\n\np\nro\nce\nss\n\nfo\nr\n\nu\nn\nd\ner\nre\np\nre\nse\nn\nte\nd\n\nIT\np\nro\nfe\nss\nio\nn\nal\ns\n\nB\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nIn\nfo\nrm\n\nat\nio\nn\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nH\nu\nm\nan\n\nex\np\ner\nti\nse\n\nis\nst\nil\nl\n\nn\nec\nes\nsa\nry\n\nE\nv\nen\n\nw\nel\nl-\nin\nte\nn\nti\no\nn\ned\n\nal\ng\no\nri\nth\nm\ns\nar\ne\nn\no\nt\n\nn\neu\ntr\nal\n\nan\nd\nsh\no\nu\nld\n\nb\ne\n\nau\nd\nit\ned\n\nfo\nr\nm\no\nra\nll\ny\n\nan\nd\nle\ng\nal\nly\n\nu\nn\nac\nce\np\nta\nb\nle\n\nd\nec\nis\nio\nn\ns\n\nU\nS\nA\n\nBusiness Research (2020) 13:795–848 813\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nR\nag\nh\nav\nan\n\net\nal\n.\n\n(2\n0\n2\n0\n)\n\nD\no\ncu\nm\nen\nts\nan\nd\n\nan\nal\ny\nze\ns\nth\ne\n\ncl\nai\nm\ns\nan\nd\n\np\nra\nct\nic\nes\n\no\nf\n\nco\nm\np\nan\nie\ns\n\no\nff\ner\nin\ng\n\nal\ng\no\nri\nth\nm\ns\nfo\nr\n\nem\np\nlo\ny\nm\nen\nt\n\nas\nse\nss\nm\nen\nt\n\nB\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nca\nse\n\nst\nu\nd\ny\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nE\nm\np\nlo\ny\nm\nen\nt\n\nas\nse\nss\nm\nen\nt\n\nS\nel\nec\nti\no\nn\n\nT\nar\ng\net\n\nv\nar\nia\nb\nle\ns\nan\nd\n\ntr\nai\nn\nin\ng\nd\nat\na:\n\nm\no\nst\no\nf\n\nth\ne\nv\nen\nd\no\nrs\n\no\nff\ner\n\ncu\nst\no\nm\niz\nab\nle\n\nas\nse\nss\nm\nen\nt\n\nV\nal\nid\nat\nio\nn\n:\nv\nen\nd\no\nr’\ns\n\nw\neb\nsi\nte\ns\no\nft\nen\n\nd\no\nn\no\nt\n\ncl\nar\nif\ny\nw\nh\net\nh\ner\n\nth\ney\n\nv\nal\nid\nat\ne\nth\nei\nr\nm\no\nd\nel\ns\n\nS\nev\ner\nal\n\nco\nu\nn\ntr\nie\ns\n\nS\nán\nch\nez\n-\n\nM\no\nn\ned\ner\no\net\n\nal\n.\n\n(2\n0\n2\n0\n)\n\nE\nx\nam\n\nin\nes\n\nh\no\nw\nth\nre\ne\n\nau\nto\nm\nat\ned\n\nh\nir\nin\ng\n\nsy\nst\nem\n\ns\nca\nn\nb\ne\n\nu\nn\nd\ner\nst\nan\nd\nan\nd\n\nat\nte\nm\np\nt\nto\n\nm\nit\nig\nat\ne\nb\nia\ns\nan\nd\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\nin\n\nth\ne\nU\nK\n\nB\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nca\nse\n\nst\nu\nd\ny\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nE\nm\np\nlo\ny\nm\nen\nt\n\nas\nse\nss\nm\nen\nt\n\nS\nel\nec\nti\no\nn\n\nO\nft\nen\n\nla\nck\n\no\nf\n\nin\nfo\nrm\n\nat\nio\nn\no\nn\nh\no\nw\n\nth\ne\nsy\nst\nem\n\nw\no\nrk\ned\n\nC\nla\nim\n\ns\nan\nd\nv\nal\nid\nat\nio\nn\n\nar\ne\no\nft\nen\n\nv\nag\nu\ne\n\nU\nK\n\n814 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nS\nto\nn\ne\net\nal\n.\n(2\n0\n1\n5\n)\n\nR\nev\nie\nw\ns\nse\nv\ner\nal\n\no\nf\n\nth\ne\np\nri\nm\nar\ny\n\nfo\nrc\nes\n\nth\nat\n\nar\ne\n\np\nre\nse\nn\nti\nn\ng\n\nch\nal\nle\nn\ng\nes\n\nfo\nr\n\nH\nR\nre\nse\nar\nch\n\nan\nd\n\np\nra\nct\nic\ne\n\nT\nri\nes\n\nto\nan\nsw\n\ner\nth\ne\n\nq\nu\nes\nti\no\nn\nw\nh\net\nh\ner\n\neH\nR\nM\n\nin\nfl\nu\nen\nce\ns\n\no\nrg\nan\niz\nat\nio\nn\nal\n\nef\nfe\nct\niv\nen\nes\ns\nan\nd\n\nw\nh\net\nh\ner\n\nit\nen\nab\nle\ns\n\no\nrg\nan\niz\nat\nio\nn\ns\nto\n\nac\nh\nie\nv\ne\nth\nei\nr\nH\nR\n\ng\no\nal\ns\n\nB\n,\nP\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nS\nev\ner\nal\n\nel\nec\ntr\no\nn\nic\n\nH\nR\nM\n\nto\no\nls\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt\n\nT\nh\ner\ne\nar\ne\nst\nil\nl\na\nn\nu\nm\nb\ner\n\no\nf\nq\nu\nes\nti\no\nn\ns\nab\no\nu\nt\n\nw\nh\net\nh\ner\n\nth\nes\ne\nn\new\n\nsy\nst\nem\n\ns\nen\nab\nle\n\no\nrg\nan\niz\nat\nio\nn\ns\nto\n\nac\nh\nie\nv\ne\nth\nei\nr\np\nri\nm\nar\ny\n\nH\nR\ng\no\nal\ns\n\nU\nS\nA\n\nW\no\no\nd\nru\nff\net\n\nal\n.\n\n(2\n0\n1\n8\n)\n\nE\nx\np\nlo\nre\ns\nh\no\nw\n\nm\nem\n\nb\ner\ns\no\nf\n\np\no\nte\nn\nti\nal\nly\n\naf\nfe\nct\ned\n\nco\nm\nm\nu\nn\nit\nie\ns\nin\n\nth\ne\nU\nS\nA\n\nfe\nel\n\nab\no\nu\nt\nal\ng\no\nri\nth\nm\nic\n\nfa\nir\nn\nes\ns\n\nB\n,\nP\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nw\no\nrk\nsh\no\np\n;\n\nin\nte\nrv\nie\nw\ns\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nC\no\nn\nce\np\nt\no\nf\nal\ng\no\nri\nth\nm\nic\n\nfa\nir\nn\nes\ns\nis\n\nla\nrg\nel\ny\n\nu\nn\nfa\nm\nil\nia\nr\n\nL\nea\nrn\nin\ng\nab\no\nu\nt\n\nal\ng\no\nri\nth\nm\nic\n\n(u\nn\n)f\nai\nrn\nes\ns\nel\nic\nit\ned\n\nn\neg\nat\niv\ne\nfe\nel\nin\ng\ns\n\nC\no\nm\np\nan\ny\nh\nan\nd\nli\nn\ng\no\nf\n\nal\ng\no\nri\nth\nm\nic\n\nfa\nir\nn\nes\ns\n\nin\nte\nra\nct\ns\nsi\ng\nn\nifi\nca\nn\ntl\ny\n\nw\nit\nh\nu\nse\nr\ntr\nu\nst\n\nU\nS\nA\n\nBusiness Research (2020) 13:795–848 815\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nL\nei\nch\nt-\nD\neo\nb\nal\nd\n\net\nal\n.\n(2\n0\n1\n9\n)\n\nId\nen\nti\nfi\nes\n\nch\nal\nle\nn\ng\nes\n\nar\nis\nin\ng\nfr\no\nm\n\nal\ng\no\nri\nth\nm\n-b\nas\ned\n\nH\nR\nd\nec\nis\nio\nn\n-\n\nm\nak\nin\ng\n\nA\nn\nal\ny\nze\ns\nh\no\nw\n\nal\ng\no\nri\nth\nm\n-b\nas\ned\n\nH\nR\n\nD\nec\nis\nio\nn\n-m\n\nak\nin\ng\n\nm\nay\n\nin\nfl\nu\nen\nce\n\nem\np\nlo\ny\nee\ns’\n\np\ner\nso\nn\nal\n\nin\nte\ng\nri\nty\n,\n\nan\nd\nac\nti\no\nn\ns\n\nB\n,\nP\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n\nD\nev\nel\no\np\nm\nen\nt\n\nA\nlg\no\nri\nth\nm\n-b\nas\ned\n\nd\nec\nis\nio\nn\n-m\n\nak\nin\ng\nca\nn\n\nh\nel\np\nm\no\nn\nit\no\nr\n\nem\np\nlo\ny\nee\ns\nm\no\nre\n\nef\nfe\nct\niv\nel\ny\nb\nu\nt\nca\nn\nb\ne\n\net\nh\nic\nal\nly\n\np\nro\nb\nle\nm\nat\nic\n\nS\nu\ng\ng\nes\nts\n\nfo\nu\nr\n\nm\nec\nh\nan\nis\nm\ns\nto\n\nre\nd\nu\nce\n\nn\neg\nat\niv\ne\nco\nn\nse\nq\nu\nen\nce\ns\n\nN\no\nt sp\nec\nifi\ned\n\nT\nam\n\nb\ne\net\n\nal\n.\n\n(2\n0\n1\n9\n)\n\nId\nen\nti\nfi\nes\n\nch\nal\nle\nn\ng\nes\n\nin\nu\nsi\nn\ng\nd\nat\na\n\nsc\nie\nn\nce\n\nte\nch\nn\niq\nu\nes\n\nfo\nr\n\nH\nR\nta\nsk\ns\n\nP\nro\np\no\nse\ns\np\nra\nct\nic\nal\n\nre\nsp\no\nn\nse\ns\nto\n\nth\nes\ne\n\nch\nal\nle\nn\ng\nes\n\nb\nas\ned\n\no\nn\nth\ne\np\nri\nn\nci\np\nle\ns\n\nca\nu\nsa\nl\nre\nas\no\nn\nin\ng\n,\n\nra\nn\nd\no\nm\niz\nat\nio\nn\nan\nd\n\nex\np\ner\nim\n\nen\nts\n,\nan\nd\n\nem\np\nlo\ny\nee\n\nco\nn\ntr\nib\nu\nti\no\nn\n\nB\n,\nP\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nw\no\nrk\nsh\no\np\n\nan\nd\nsu\nrv\ney\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nF\no\nu\nr\nch\nal\nle\nn\ng\nes\n\nid\nen\nti\nfi\ned\n:\nco\nm\np\nle\nx\nit\ny\n\no\nf\nH\nR\n\np\nh\nen\no\nm\nen\na,\n\nco\nn\nst\nra\nin\nts\nim\n\np\no\nse\nd\n\nb\ny\nsm\n\nal\nl\nd\nat\na\nse\nts\n,\n\nac\nco\nu\nn\nta\nb\nil\nit\ny\n\nq\nu\nes\nti\no\nn\ns\nas\nso\nci\nat\ned\n\nw\nit\nh\nfa\nir\nn\nes\ns\nan\nd\n\no\nth\ner\n\net\nh\nic\nal\n\nan\nd\nle\ng\nal\n\nco\nn\nst\nra\nin\nts\n,\nan\nd\n\np\no\nss\nib\nle\n\nad\nv\ner\nse\n\nem\np\nlo\ny\nee\n\nre\nac\nti\no\nn\ns\n\nU\nS\nA\n\n816 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nR\no\nse\nn\nb\nla\nt\net\n\nal\n.\n\n(2\n0\n1\n4\n)\n\nF\no\ncu\nse\ns\no\nn\nth\ne\nn\new\n\nto\no\nl’\ns\nem\n\np\nlo\ny\ner\ns\n\nu\nse\n\nto\nsi\nft\nth\nro\nu\ng\nh\n\njo\nb\nap\np\nli\nca\nti\no\nn\ns\n\nA\nd\nd\nre\nss\nes\n\nis\nsu\nes\n\no\nf\n\np\nri\nv\nac\ny\n,\nfa\nir\nn\nes\ns,\n\ntr\nan\nsp\nar\nen\ncy\n,\n\nac\ncu\nra\ncy\n,\nan\nd\n\nin\neq\nu\nal\nit\ny\nu\nn\nd\ner\n\nth\ne\nru\nb\nri\nc\no\nf\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nli\nte\nra\ntu\nre\n\nre\nv\nie\nw\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nE\nm\np\nlo\ny\ner\ns\np\no\nte\nn\nti\nal\nly\n\nh\nav\ne\nac\nce\nss\n\nto\nm\no\nre\n\nco\nm\np\nre\nh\nen\nsi\nv\ne\n\nel\nec\ntr\no\nn\nic\n\np\nro\nfi\nle\ns\no\nn\n\njo\nb\nca\nn\nd\nid\nat\nes\n\nth\nan\n\nh\nas\n\nb\nee\nn\ntr\nad\nit\nio\nn\nal\nly\n\nav\nai\nla\nb\nle\n\nto\nth\nem\n\n,\n\nw\nh\nic\nh\nca\nn\nex\np\no\nse\n\njo\nb\n\nca\nn\nd\nid\nat\nes\n\nto\na\ng\nre\nat\ner\n\nsc\nru\nti\nn\ny\no\nf\nth\nei\nr\n\np\ner\nso\nn\nal\n\nli\nv\nes\n\nN\no\nt sp\nec\nifi\ned\n\nB\nu\nrd\no\nn\nan\nd\n\nH\nar\np\nu\nr\n(2\n0\n1\n5\n)\n\nE\nx\nam\n\nin\nes\n\np\no\nte\nn\nti\nal\n\nfo\nr\n\nd\nis\ncr\nim\n\nin\nat\no\nry\n\np\nra\nct\nic\nes\n\nto\n\nd\nev\nel\no\np\nth\nro\nu\ng\nh\n\nin\nfo\nrm\n\nat\nio\nn\n\nin\nfr\nas\ntr\nu\nct\nu\nre\ns\nin\n\nw\nh\nic\nh\nu\nn\nfa\nir\nn\nes\ns\n\nan\nd\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nar\ne\nem\n\nb\ned\nd\ned\n\nin\nto\n\nth\ne\n\np\nre\nsc\nri\np\nti\nv\ne\n\np\nro\nce\nss\nes\n\nan\nd\n\nin\nfr\nas\ntr\nu\nct\nu\nre\ns\no\nf\n\nta\nle\nn\nt\nan\nal\ny\nti\ncs\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nL\naw\n\nG\nen\ner\nal\n\nS\nel\nec\nti\no\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt\n\nP\nro\nce\nss\nes\n\no\nf\np\nre\nd\nic\nti\nv\ne\n\nse\ng\nm\nen\nta\nti\no\nn\nca\nn\n\np\nro\nd\nu\nce\n\nin\neq\nu\nal\nit\nie\ns\n\nth\nro\nu\ng\nh\nth\ne\n\nse\ng\nm\nen\nta\nti\no\nn\no\nf\n\nem\np\nlo\ny\nee\n\ng\nro\nu\np\nin\ng\ns\n\nb\nas\ned\n\no\nn\nu\nn\nin\ntu\nit\niv\ne\n\nat\ntr\nib\nu\nte\ns\nth\nat\n\nar\ne\n\nev\ner\n-c\nh\nan\ng\nin\ng\n\nA\nu\nst\nra\nli\na\n\nBusiness Research (2020) 13:795–848 817\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nP\ner\nss\no\nn\n(2\n0\n1\n6\n)\n\nA\nn\nal\ny\nze\ns\nth\ne\n\np\nro\nb\nle\nm\ns\no\nf\n\nim\np\nli\nci\nt\nb\nia\ns\nin\n\nal\ng\no\nri\nth\nm\ns\n\nre\ng\nar\nd\nin\ng\n\nre\ncr\nu\nit\nm\nen\nt\n\np\nro\nce\nss\nes\n,\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n,\n\nan\nd\nu\nn\nfa\nir\nn\nes\ns\nb\ny\n\nu\nsi\nn\ng\nal\ng\no\nri\nth\nm\ns\n\nS\nh\no\nw\ns\np\no\nss\nib\nle\n\nso\nlu\nti\no\nn\ns\n\nB\n,\nD\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nex\np\nlo\nra\nti\nv\ne\n\nan\nal\ny\nsi\ns\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nD\nat\na\nm\nin\nin\ng\n,\n\np\nro\nfi\nli\nn\ng\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nE\nm\np\nlo\ny\ner\ns\nm\nig\nh\nt\nm\nis\ns\n\nth\ne\nb\nes\nt\nca\nn\nd\nid\nat\nes\n,\nas\n\nth\ne\nem\n\np\nlo\ny\ned\n\nal\ng\no\nri\nth\nm\ns\nar\ne\ntu\nn\ned\n\nw\nit\nh\nli\nm\nit\ned\n\nan\nd\n\no\nu\ntd\nat\ned\n\nd\nat\na\n\nT\nh\ne\nri\nsk\n\no\nf\nd\nir\nec\ntl\ny\no\nr\n\nin\nd\nir\nec\ntl\ny\n\nd\nis\ncr\nim\n\nin\nat\nin\ng\n\nca\nn\nd\nid\nat\nes\n\nex\nis\nts\n\nN\no\nt sp\nec\nifi\ned\n\nV\nas\nco\nn\nce\nlo\ns\net\n\nal\n.\n\n(2\n0\n1\n7\n)\n\nP\nro\np\no\nse\ns\na\n\nst\nru\nct\nu\nre\nd\n\nap\np\nro\nac\nh\nto\n\nm\nit\nig\nat\ne\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nan\nd\nu\nn\nfa\nir\nn\nes\ns\n\nca\nu\nse\nd\nb\ny\nb\nia\ns\nin\n\nA\nI\nsy\nst\nem\n\ns\nin\n\nh\nir\nin\ng\nd\nec\nis\nio\nn\n\nsc\nen\nar\nio\ns\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nG\nen\ner\nal\n,\nh\nir\nin\ng\n\nal\ng\no\nri\nth\nm\ns\n\nS\nel\nec\nti\no\nn\n\nP\no\nin\nts\no\nu\nt\nco\nn\nn\nec\nti\no\nn\ns\n\nb\net\nw\nee\nn\nb\nia\ns\no\nf\nA\nI\n\nan\nd\nth\ne\np\nro\nb\nle\nm\n\no\nf\n\nin\nd\nu\nct\nio\nn\n\nS\nh\no\nw\ns\nth\nat\n\nth\ner\ne\nis\n\na\n\nlo\ng\nic\nal\n\nth\neo\nry\n\no\nf\n\np\nre\nfe\nre\nn\nce\ns\n\nN\no\nt sp\nec\nifi\ned\n\n818 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nC\nh\nen\n\net\nal\n.\n(2\n0\n1\n8\n)\n\nIn\nv\nes\nti\ng\nat\nes\n\ng\nen\nd\ner\n-\n\nb\nas\ned\n\nin\neq\nu\nal\nit\nie\ns\n\nin\nth\ne\nco\nn\nte\nx\nt\no\nf\n\nre\nsu\nm\ne\nse\nar\nch\n\nen\ng\nin\nes\n,\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nB\n,\nD\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nan\nti\nta\nti\nv\ne;\n\nst\nat\nis\nti\nca\nl\n\nte\nst\ns\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nS\nea\nrc\nh\nen\ng\nin\nes\n\nR\nec\nru\nit\nm\nen\nt\n\nIn\nd\niv\nid\nu\nal\n\nfa\nir\nn\nes\ns:\nev\nen\n\nw\nh\nen\n\nco\nn\ntr\no\nll\nin\ng\nfo\nr\n\nal\nl\no\nth\ner\n\nv\nis\nib\nle\n\nca\nn\nd\nid\nat\ne\nfe\nat\nu\nre\ns,\n\nth\ner\ne\nis\n\na\nsl\nig\nh\nt\n\np\nen\nal\nty\n\nag\nai\nn\nst\nfe\nm\nal\ne\n\nca\nn\nd\nid\nat\nes\n\nG\nro\nu\np\nfa\nir\nn\nes\ns:\n\n8\n.5\n–\n1\n3\n.2\n%\n\no\nf\njo\nb\nti\ntl\ne/\n\nci\nty\n\np\nai\nrs\n\nsh\no\nw\n\nst\nat\nis\nti\nca\nll\ny\nsi\ng\nn\nifi\nca\nn\nt\n\ng\nro\nu\np\nu\nn\nfa\nir\nn\nes\ns\n\nU\nS\nA\n\nB\no\ng\nen\n\n(2\n0\n1\n9\n)\n\nA\nn\nal\ny\nsi\ns\no\nf\n\np\nre\nd\nic\nti\nv\ne\nto\no\nls\n\nac\nro\nss\n\nth\ne\nh\nir\nin\ng\n\np\nro\nce\nss\n\nto\ncl\nar\nif\ny\n\nw\nh\nat\n\nh\nir\nin\ng\n\nal\ng\no\nri\nth\nm\ns\nd\no\n,\n\nan\nd\nw\nh\ner\ne\nan\nd\n\nh\no\nw\n\nb\nia\ns\nca\nn\n\nen\nte\nr\nin\nto\n\nth\ne\n\np\nro\nce\nss\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n,\nh\nir\nin\ng\n\nal\ng\no\nri\nth\nm\ns\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nM\no\nst\nh\nir\nin\ng\nal\ng\no\nri\nth\nm\ns\n\nw\nil\nl\nd\nri\nft\nto\nw\nar\nd\nb\nia\ns\n\nb\ny\nd\nef\nau\nlt\n\nP\no\nte\nn\nti\nal\n\nto\nh\nel\np\nre\nd\nu\nce\n\nin\nte\nrp\ner\nso\nn\nal\n\nb\nia\ns\n\nsh\no\nu\nld\n\nn\no\nt\nb\ne\n\nd\nis\nco\nu\nn\nte\nd\n\nO\nn\nly\n\nto\no\nls\nth\nat\n\np\nro\nac\nti\nv\nel\ny\nta\nck\nle\n\nd\nee\np\ner\n\nd\nis\np\nar\nit\nie\ns\nw\nil\nl\n\no\nff\ner\n\nan\ny\nh\no\np\ne\nth\nat\n\np\nre\nd\nic\nti\nv\ne\nte\nch\nn\no\nlo\ng\ny\n\nca\nn\nh\nel\np\np\nro\nm\no\nte\n\neq\nu\nit\ny\n,\nra\nth\ner\n\nth\nan\n\ner\no\nd\ne\nit\n\nU\nS\nA\n\nBusiness Research (2020) 13:795–848 819\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nS\nim\n\nb\nec\nk\n(2\n0\n1\n9\n)\n\nD\nis\ncu\nss\nes\n\nth\ne\n\net\nh\nic\nal\n\nim\np\nli\nca\nti\no\nn\ns\no\nf\n\nth\ne\nap\np\nli\nca\nti\no\nn\no\nf\n\nso\np\nh\nis\nti\nca\nte\nd\n\nan\nal\ny\nti\nca\nl\n\nm\net\nh\no\nd\ns\nto\n\nq\nu\nes\nti\no\nn\ns\nin\n\nH\nR\n\nm\nan\nag\nem\n\nen\nt\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt\n\nT\nh\ne\nri\nsi\nn\ng\nd\nat\na\no\nn\n\nem\np\nlo\ny\nee\ns\nw\nil\nl\n\nL\nea\nd\nto\n\nan\n\nu\nn\np\nre\nce\nd\nen\nte\nd\n\ntr\nan\nsp\nar\nen\ncy\n\no\nf\n\nem\np\nlo\ny\nee\ns\n\nP\nro\np\no\nse\ns\nto\n\ntr\nan\nsf\ner\n\nk\ney\n\net\nh\nic\nal\n\nco\nn\nce\np\nts\n\nfr\no\nm\n\nm\ned\nic\nal\n\nre\nse\nar\nch\n,\n\nar\nti\nfi\nci\nal\n\nin\nte\nll\nig\nen\nce\n,\n\nle\nar\nn\nin\ng\nan\nal\ny\nti\ncs\n,\nan\nd\n\nco\nac\nh\nin\ng\nto\n\nH\nR\n\nan\nal\ny\nti\ncs\n\nN\no\nt sp\nec\nifi\ned\n\n820 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nK\nel\nlo\ng\ng\net\n\nal\n.\n\n(2\n0\n2\n0\n)\n\nA\nn\nal\ny\nze\ns\nh\no\nw\n\nth\ne\n\nim\np\nle\nm\nen\nta\nti\no\nn\n\no\nf\nal\ng\no\nri\nth\nm\nic\n\nte\nch\nn\no\nlo\ng\nie\ns\nin\n\no\nrg\nan\niz\nat\nio\nn\ns\n\nm\nay\n\nre\nsh\nap\ne\n\no\nrg\nan\niz\nat\nio\nn\nal\n\nco\nn\ntr\no\nl\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nli\nte\nra\ntu\nre\n\nre\nv\nie\nw\n\nM\nan\nag\nem\n\nen\nt\n\nA\nlg\no\nri\nth\nm\nic\n\nre\nco\nm\nm\nen\nd\nin\ng\n,\n\nre\nst\nri\nct\nin\ng\n,\n\nre\nco\nrd\nin\ng\n,\n\nra\nti\nn\ng\n,\n\nre\np\nla\nci\nn\ng\n,\nan\nd\n\nre\nw\nar\nd\nin\ng\n\nD\nev\nel\no\np\nm\nen\nt\n\nA\nlg\no\nri\nth\nm\nic\n\nco\nn\ntr\no\nl\nin\n\nth\ne\nw\no\nrk\np\nla\nce\n\no\n",
        "metadata_storage_path": "aHR0cHM6Ly9kZWZhdWx0cmVzb3VyY2Vncm91YWFkMy5ibG9iLmNvcmUud2luZG93cy5uZXQvY3VyYXRlZC1saWJyYXJ5L0tvJUNDJTg4Y2hsaW5nLVdlaG5lcjIwMjBfQXJ0aWNsZV9EaXNjcmltaW5hdGVkQnlBbkFsZ29yaXRobUFTeXMucGRm0",
        "metadata_content_type": "application/pdf",
        "metadata_language": "en",
        "metadata_author": "Alina Köchling ",
        "metadata_title": "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development",
        "metadata_creation_date": "2020-11-19T15:45:16Z",
        "organizations": [
          "human resource management",
          "HRM",
          "Administration",
          "Economics",
          "Heinrich-Heine-University Düsseldorf",
          "Möhlmann",
          "Deloitte",
          "Google",
          "IBM",
          "SAP",
          "Microsoft",
          "Vodafone",
          "Intel",
          "Unilever",
          "Ikea",
          "Daugherty",
          "Arrow",
          "Kim",
          "Amazon",
          "Dastin",
          "Citron",
          "Pasquale",
          "company",
          "workplace",
          "Kaplan",
          "Haenlein",
          "department",
          "fairness",
          "Group",
          "opportunity",
          "Business Research",
          "Bauer",
          "procedural justice",
          "fairness measures",
          "EBSCO",
          "AOM",
          "EURAM",
          "ACM",
          "IEEE",
          "SSCI",
          "applied",
          "science interdisciplinary applications",
          "humanities interdisciplinary",
          "Hoffmann",
          "robustness",
          "cl",
          "ib",
          "Journal of Business",
          "Academy of",
          "ec",
          "ss",
          "ag",
          "ir",
          "sp",
          "sw",
          "ifi",
          "ap",
          "rp",
          "fe",
          "rc"
        ],
        "people": [
          "Alina Köchling1",
          "Marius Claus Wehner1",
          "Alina Köchling",
          "Zalmanson",
          "Chalfin",
          "Lindebaum",
          "Silverman",
          "Waller",
          "Carey",
          "Smith",
          "Savage",
          "Bales",
          "Walker",
          "Wilson",
          "Suen",
          "McDonald",
          "McColl",
          "Michelotti",
          "Woods",
          "Raghavan",
          "Lepri",
          "Lee",
          "Simbeck",
          "Barocas",
          "Selbst",
          "Suresh",
          "Guttag",
          "Chander",
          "Dwork",
          "Miller",
          "Pasquale",
          "Kaibel",
          "Langer",
          "Huselid",
          "Ötting",
          "Maier",
          "Tambe",
          "Cappelli",
          "Möhlmann",
          "Russell",
          "Norvig",
          "Paschen",
          "Shin",
          "Murphy",
          "Kael",
          "bling",
          "Bengio",
          "Deng",
          "Yu",
          "Kauermann",
          "Kuechenhoff",
          "Goodfellow",
          "Kahneman",
          "Friedman",
          "Nissenbaum",
          "Cheng",
          "Hackett",
          "Roscher",
          "Danks",
          "Diakopoulos",
          "Veale",
          "Datta",
          "Barfield",
          "Pagallo",
          "Wolpert",
          "Leventhal",
          "Hardt",
          "Persson",
          "Bertrand",
          "Frijters",
          "Cascio",
          "Aguinis",
          "Meade",
          "Fetzer",
          "Roth",
          "Bobko",
          "Bartlett",
          "Cropanzano",
          "Cohen-Charash",
          "Spector",
          "Gilliland",
          "McCarthy",
          "Hausknecht",
          "Petticrew",
          "Roberts",
          "Crossan",
          "Apaydin",
          "Siddaway",
          "Gough",
          "Lipsey",
          "Ferguson",
          "Brannick",
          "Moher",
          "Yarger",
          "Varghese",
          "Horton",
          "Gil-Lafuente",
          "Oh",
          "Podsakoff",
          "Morrison",
          "jo",
          "li",
          "rk"
        ],
        "keyphrases": [
          "200 artificial intelligence (AI) specialists",
          "two essential HR functions",
          "Marius Claus Wehner1",
          "Heinrich-Heine-University Düsseldorf",
          "major driving forces",
          "human resource management",
          "Alina Köchling1",
          "routinized workplace decisions",
          "current human resource",
          "crucial future research",
          "HRM � Literature review",
          "Abstract Algorithmic decision-making",
          "Keywords Fairness � Discrimination",
          "human biases",
          "systematic review",
          "HR recruitment",
          "HR development",
          "HR) practices",
          "ORIGINAL RESEARCH",
          "new source",
          "unfair treatment",
          "implicit discrimination",
          "implicit) discrimination",
          "current state",
          "research gaps",
          "36 journal articles",
          "possible pitfalls",
          "important theoretical",
          "practical implications",
          "fruitful avenues",
          "fairness � Ethics",
          "rapid growth",
          "Business Administration",
          "Universitätsstrasse",
          "Business Research",
          "automated decision-making",
          "remote control",
          "Möhlmann",
          "important individual",
          "societal implications",
          "organizational optimization",
          "hidden talented",
          "large number",
          "German companies",
          "competitive advantages",
          "commercial providers",
          "algorithmic platforms",
          "performance measurements",
          "large companies",
          "economic reasons",
          "personal beliefs",
          "doi.org",
          "orcid.org",
          "context",
          "Author",
          "advice",
          "firms",
          "costs",
          "efficiency",
          "objectivity",
          "groups",
          "people",
          "unfairness",
          "knowledge",
          "threats",
          "goal",
          "directions",
          "applications",
          "researchers",
          "practitioners",
          "1 Introduction",
          "information",
          "importance",
          "digitalization",
          "organizations",
          "koechling",
          "hhu",
          "1 Faculty",
          "Economics",
          "40225 Dusseldorf",
          "Germany",
          "crossmark",
          "crossref",
          "standardization",
          "Zalmanson",
          "Algorithms",
          "humans",
          "Chalfin",
          "Lee",
          "Lindebaum",
          "changes",
          "favor",
          "employees",
          "Silverman",
          "Waller",
          "Carey",
          "Smith",
          "Savage",
          "Bales",
          "survey",
          "Deloitte",
          "Several",
          "Google",
          "IBM",
          "SAP",
          "Microsoft",
          "systems",
          "hiring",
          "Walker",
          "turn",
          "Vodafone",
          "Unilever",
          "Ikea",
          "Daugherty",
          "Wilson",
          "Precire",
          "savings",
          "time",
          "risks",
          "productivity",
          "certainty",
          "Suen",
          "McDonald",
          "McColl",
          "Michelotti",
          "Woods",
          "prejudices",
          "15",
          "American e-commerce specialist",
          "algorithmic decision- making",
          "new research avenues",
          "HR development processes",
          "unrepresentative input data",
          "algorithmic decision-making system",
          "complete algorithmic decision-making",
          "training) data",
          "algorithmic outcomes",
          "same attention",
          "same requirements",
          "first glance",
          "human decision-making",
          "unequal treatment",
          "different groups",
          "qualitative differences",
          "individual performance",
          "biased outcomes",
          "biased decisions",
          "current debate",
          "extreme disadvantage",
          "796 Business Research",
          "Previous studies",
          "common procedures",
          "labor shortages",
          "one hand",
          "other side",
          "computer science",
          "neous state",
          "distinct challenges",
          "future research",
          "practical point",
          "known companies",
          "negative consequences",
          "potential downsides",
          "potential dangers",
          "possible threat",
          "prominent example",
          "female applicants",
          "hiring decision",
          "employees’ acceptance",
          "existing knowledge",
          "current literature",
          "hiring algorithms",
          "potential biases",
          "HRM context",
          "consistency",
          "fairness",
          "Langer",
          "Florentine",
          "Raghavan",
          "application",
          "criteria",
          "Lepri",
          "discrimination",
          "Simbeck",
          "gender",
          "ethnicity",
          "Arrow",
          "Kim",
          "Barocas",
          "Selbst",
          "Suresh",
          "Guttag",
          "Chander",
          "issue",
          "transparency",
          "Dwork",
          "Diakopoulos",
          "Amazon",
          "Dastin",
          "Miller",
          "lack",
          "accountability",
          "factors",
          "Citron",
          "Pasquale",
          "question",
          "Kaibel",
          "discrepancy",
          "enthusiasm",
          "panacea",
          "inefficiencies",
          "field",
          "infancy",
          "digitization",
          "automation",
          "view",
          "aim",
          "study",
          "awareness",
          "The Oxford Living Dictionary",
          "two important HR areas",
          "several different research areas",
          "various algorithmic decision-making tools",
          "several AI decision tools",
          "other problem-solving operations",
          "two HR functions",
          "explicit human interference",
          "natural language processing",
          "several research gaps",
          "future research avenues",
          "systematic literature review",
          "HR development methods",
          "future research directions",
          "several ways",
          "workplace decision",
          "human intelligence",
          "algorithmic selection",
          "career development",
          "future performance",
          "person-organization fit",
          "serious consequences",
          "Ötting",
          "existing body",
          "ethical issues",
          "illustrative examples",
          "timely topic",
          "enormous importance",
          "reputational risk",
          "ment process",
          "key terms",
          "descriptive analysis",
          "subsequent discussion",
          "theoretical implications",
          "Conceptual background",
          "computational mechanism",
          "umbrella term",
          "wide array",
          "machine learning",
          "image recognition",
          "existing literature",
          "AI applications",
          "current employees",
          "HRM field",
          "lacking focus",
          "statistical models",
          "new data",
          "present paper",
          "distributive fairness",
          "understanding",
          "end",
          "potential",
          "Huselid",
          "Decisions",
          "algorithms",
          "individuals",
          "company",
          "society",
          "ethics",
          "procedural",
          "Maier",
          "Tambe",
          "Cappelli",
          "bias",
          "fact",
          "Companies",
          "legal",
          "applicants",
          "reason",
          "guidance",
          "detailed",
          "definitions",
          "methodology",
          "illustration",
          "processes",
          "sets",
          "rules",
          "calculations",
          "computer",
          "routinized",
          "basis",
          "prescriptions",
          "acquisition",
          "environment",
          "Russell",
          "Norvig",
          "ML",
          "speech",
          "NLP",
          "Kaplan",
          "Haenlein",
          "Paschen",
          "2.1",
          "many AI decision-making tools",
          "three major types",
          "artificial neural networks",
          "random variable Y",
          "three core elements",
          "three different levels",
          "neural network models",
          "deep learning models",
          "fixed input/output data",
          "reinforcement type learning",
          "Unsupervised ML algorithms",
          "reinforcement learning",
          "ML models",
          "network structures",
          "probabilistic models",
          "unsupervised settings",
          "learning tasks",
          "ML approach",
          "regression-type problems",
          "ground truth",
          "Human experts",
          "human decisions",
          "future instances",
          "same problem",
          "priori labeling",
          "structural behaviors",
          "theme analysis",
          "798 Business Research",
          "separate group",
          "error interactions",
          "dynamic environment",
          "Single nodes",
          "human brain",
          "human thinking",
          "other settings",
          "prediction tasks",
          "expected values",
          "systematic error",
          "human judgment",
          "algorithmic evaluations",
          "computer systems",
          "black box",
          "glass boxes",
          "making sense",
          "human involvement",
          "Cognitive biases",
          "entire model",
          "nonlinear transformation",
          "individual components",
          "input data",
          "Shin",
          "predictions",
          "outputs",
          "labels",
          "patterns",
          "Canhoto",
          "contrast",
          "Murphy",
          "variables",
          "methods",
          "trial",
          "bling",
          "methodologies",
          "Examples",
          "complex",
          "relationship",
          "phases",
          "layers",
          "neurons",
          "Bengio",
          "multiple",
          "stages",
          "features",
          "representations",
          "advantage",
          "Deng",
          "Yu",
          "Reason",
          "estimation",
          "bY",
          "difference",
          "Kauermann",
          "Kuechenhoff",
          "Goodfellow",
          "uncertainty",
          "Kahneman",
          "others",
          "Friedman",
          "Nissenbaum",
          "HRM",
          "Cheng",
          "Hackett",
          "theory",
          "consideration",
          "distinction",
          "interpretability",
          "explainability",
          "Roscher",
          "combination",
          "former",
          "simulatability",
          "parameters",
          "2.2",
          "predictive algorithmic decision-making tool",
          "accurate algorithmic output",
          "explicit human judgments",
          "input data set",
          "historical employment data",
          "algorithmic transparency",
          "historical data",
          "ML model",
          "different sets",
          "HR department",
          "employee satisfaction",
          "turnover intentions",
          "specific criteria",
          "underlying reasons",
          "main reasons",
          "learning process",
          "exposed examples",
          "one way",
          "subsequent analysis",
          "worst case",
          "discriminatory outputs",
          "manual programming",
          "recruitment process",
          "white men",
          "protected group",
          "systematic disadvantage",
          "biased data",
          "historical biases",
          "different biases",
          "preexisting biases",
          "Contextual information",
          "emergent bias",
          "representation bias",
          "specific information",
          "personality characteristics",
          "ML algorithm",
          "valid example",
          "implicit bias",
          "decomposability",
          "level",
          "training",
          "Interpretability",
          "element",
          "domain",
          "interpretations",
          "derive",
          "conclusions",
          "results",
          "prediction",
          "interest",
          "technical",
          "quality",
          "Danks",
          "London",
          "keyword",
          "garbage",
          "stereotypes",
          "Barfield",
          "Pagallo",
          "creation",
          "kind",
          "racist",
          "Veale",
          "Binns",
          "tendencies",
          "past",
          "Datta",
          "Hispanics",
          "applicant",
          "member",
          "job",
          "interview",
          "designer",
          "categories",
          "disadvantages",
          "absence",
          "selection",
          "measurements",
          "information systems Leventhal",
          "new societal knowledge",
          "different fairness definitions",
          "new knowledge",
          "several steps",
          "feature engineering",
          "free lunch",
          "discriminatory outcomes",
          "technical constraints",
          "technical consideration",
          "several reasons",
          "important conditions",
          "human constructs",
          "human interpretation",
          "cultural values",
          "two reasons",
          "different values",
          "design process",
          "cooperative efforts",
          "competitive behavior",
          "equal treatment",
          "meso- level",
          "statistical parity",
          "true outcomes",
          "fair treatment",
          "equal opportunity",
          "individual subjects",
          "representative bias",
          "Technical bias",
          "algorithmic system",
          "system design",
          "measurement data",
          "model testing",
          "best model",
          "800 Business Research",
          "training data",
          "Individual fairness",
          "group membership",
          "computer technology",
          "specific context",
          "cultural context",
          "group fairness",
          "real users",
          "protected) groups",
          "preprocessing",
          "theorems",
          "Wolpert",
          "Macready",
          "choice",
          "benchmark",
          "performance",
          "rotations",
          "example",
          "females",
          "comparison",
          "overrepresented",
          "limited",
          "hardware",
          "software",
          "peripherals",
          "Bozdag",
          "formalization",
          "computers",
          "problem",
          "judgments",
          "intuitions",
          "law",
          "litigation",
          "construction",
          "result",
          "population",
          "shift",
          "mismatch",
          "Table",
          "state",
          "art",
          "competition",
          "individualistic",
          "needs",
          "overview",
          "measures",
          "hand",
          "micro-level",
          "Hardt",
          "notions",
          "sense",
          "positives/negatives",
          "sources",
          "disadvantage",
          "2.3",
          "fairness Name Author Definition Individual fairness Dwork",
          "similar’’ classifications Group fairness",
          "descriptive personal evaluation",
          "considerable adverse consequences",
          "individual differences",
          "actual fairness",
          "fairness perceptions",
          "objective fairness",
          "Subjective fairness",
          "Discriminatory categories",
          "working experience",
          "conscious process",
          "current position",
          "personnel selection",
          "prediction systems",
          "selection context",
          "differential validity",
          "subgroup validity",
          "differential prediction",
          "biased results",
          "Table 1 Definitions",
          "P bY",
          "Pð bY",
          "opportunity Hardt",
          "False-negative rates",
          "1f g",
          "random variable",
          "802 Business Research",
          "selection process",
          "applicant pool",
          "training process",
          "meaningful effects",
          "negative experiences",
          "recruitment context",
          "job offer",
          "job turnover",
          "explicit discrimination",
          "selection measure",
          "essential role",
          "equal probability",
          "conscientious behavior",
          "managerial decisions",
          "Implicit discrimination",
          "implicit attitudes",
          "HR literature",
          "years",
          "Persson",
          "Bertrand",
          "aversion",
          "support",
          "characteristics",
          "Frijters",
          "Cascio",
          "Aguinis",
          "probabilities",
          "estimates",
          "slopes",
          "intercepts",
          "subgroups",
          "Meade",
          "Fetzer",
          "Roth",
          "Bobko",
          "Bartlett",
          "usage",
          "algorithmic",
          "decision-making",
          "regard",
          "justice",
          "reality",
          "Cropanzano",
          "employers",
          "subjects",
          "positive",
          "1jG",
          "recidivism",
          "estimator",
          "likelihood",
          "altruisms",
          "Cohen-Charash",
          "Spector",
          "candidates",
          "Bauer",
          "morale",
          "intentions",
          "acceptance",
          "rejection",
          "dissatisfaction",
          "reduction",
          "elimination",
          "conflicts",
          "Gilliland",
          "McCarthy",
          "Hausknecht",
          "image",
          "systematic literature review approach",
          "systematic literature reviews",
          "Several online platforms",
          "extensive keyword searching",
          "defined individual concepts",
          "three core dimensions",
          "fruitful research avenues",
          "high procedural justice",
          "three fairness dimensions",
          "three dimensions",
          "replicable approach.1",
          "individual fairness",
          "research question",
          "rating companies",
          "Van Hoye",
          "most urgency",
          "equal opportunities",
          "ethical norms",
          "important role",
          "relevant information",
          "algorithmic decision-making",
          "information processing",
          "human raters",
          "employee evaluation",
          "personal contact",
          "reliable picture",
          "discrimination potential",
          "particular field",
          "overall picture",
          "two authors",
          "Distributive justice",
          "informational justice",
          "fairness measures",
          "Search terms",
          "interactional justice",
          "development process",
          "organizational context",
          "evidence-based decisions",
          "decision process",
          "iterative process",
          "interactional fairness",
          "same treatment",
          "possibility",
          "outcome",
          "Rules",
          "equality",
          "equity",
          "accordance",
          "contributions",
          "extent",
          "Leventhal",
          "Consistency",
          "accuracy",
          "representation",
          "parties",
          "correction",
          "appropriateness",
          "dignity",
          "courtesy",
          "respect",
          "share",
          "general",
          "procedures",
          "errors",
          "3 Methods",
          "insights",
          "other",
          "suggestions",
          "Petticrew",
          "Roberts",
          "Crossan",
          "Apaydin",
          "Siddaway",
          "Gough",
          "databases",
          "discussion",
          "keywords",
          "3.1",
          "EBSCO Business Source Premier database",
          "social science citation index",
          "Search string Database Resultsa",
          "facial expression tool",
          "prone*OR justiceb",
          "facial expression processing",
          "OR data set",
          "language peer-reviewed journals",
          "data mining discrimination",
          "Human Resource Management",
          "algorithmic bias discrimination",
          "first source",
          "high-quality source",
          "804 Business Research",
          "search strings",
          "search approach",
          "search terms",
          "search engine",
          "algorithmic model",
          "human bias",
          "singular/plural forms",
          "different spellings",
          "narrow terms",
          "complete list",
          "broad coverage",
          "impact factor",
          "reasonable proxy",
          "important publications",
          "reference section",
          "book reviews",
          "editorial notes",
          "conference proceedings",
          "renowned conferences",
          "anonymous reviewer",
          "initial identification",
          "artificial intelligence",
          "recommender system",
          "demographic group",
          "equal opportunit",
          "human judgement",
          "pattern distortion",
          "adverse impactb",
          "algorithmic judgement",
          "publication bias",
          "national bias",
          "gender- bias",
          "decision-making bias",
          "technical bias",
          "algorithmic fairness",
          "mechanical judgement",
          "algorithmic discrimination",
          "classification terms",
          "eligibility process",
          "inclusion process",
          "classification problem",
          "fairness word",
          "fairness speech",
          "fairness recommendation",
          "applicant selection",
          "employee selection",
          "relevant articles",
          "scholarly articles",
          "unpublished articles",
          "scholarly literature",
          "valuable recommendation",
          "Table 2 Overview",
          "electronic databases",
          "3207 articles",
          "terminology",
          "synonyms",
          "contents",
          "priority",
          "balance",
          "sensitivity",
          "specificity",
          "unintentional",
          "omission",
          "SSCI",
          "part",
          "Web",
          "Knowledge",
          "breadth",
          "papers",
          "evidence",
          "English",
          "comments",
          "AOM",
          "EURAM",
          "ACM",
          "IEEE",
          "authors",
          "Lipsey",
          "Ferguson",
          "Brannick",
          "April",
          "Screening",
          "title",
          "recruitment",
          "development",
          "data-algorithm",
          "TOPIC",
          "sex",
          "origin",
          "ethical",
          "implication",
          "truth",
          "evaluat",
          "rate",
          "measure",
          "valuation",
          "computer science artificial intelligence",
          "EBSCO Business Source Premier",
          "computer science interdisciplinary applications",
          "operations research management science",
          "Peer Reviewed) Journals",
          "Preferred Reporting Items",
          "science information systems",
          "social sciences interdisciplinary",
          "PRISMA flow diagram",
          "actual search process",
          "multidisciplinary science",
          "behavioral science",
          "humanities interdisciplinary",
          "EBSCO search",
          "social issues",
          "PRISMA) recommendations",
          "research results",
          "research design",
          "research geography",
          "actual review",
          "personnel decision",
          "job advertisement",
          "DOCUMENT TYPES",
          "gross hits",
          "bRobustness check",
          "relevance screening",
          "eligibility stage",
          "full text",
          "structured approach",
          "high level",
          "Systematic Reviews",
          "detailed report",
          "succinct summary",
          "coherent picture",
          "two additional",
          "adverse impact",
          "application process",
          "search string",
          "HR decision-making",
          "literature review",
          "people analytic",
          "research content",
          "Academic Journal",
          "preexisting codes",
          "key findings",
          "eight articles",
          "three articles",
          "Analytic categories",
          "SSCI psychology",
          "Article English",
          "HR.",
          "102 articles",
          "632 articles",
          "690 articles",
          "online-recruitment",
          "personalization",
          "LANGUAGES",
          "sociology",
          "aResults",
          "database",
          "Studies",
          "definition",
          "list",
          "request",
          "paper",
          "reliability",
          "material",
          "set",
          "year",
          "publication",
          "assessment",
          "number",
          "records",
          "Figure 1",
          "Moher",
          "reliable",
          "abstract",
          "The",
          "majority",
          "reference",
          "3.3",
          "high-impact, peer-reviewed journals",
          "initial search process",
          "other HR functions",
          "remaining three articles",
          "current research landscape",
          "obvious discrimination context",
          "HR development context",
          "two broad",
          "HR context",
          "two articles",
          "other areas",
          "other sources",
          "two databases",
          "robustness check",
          "transparent picture",
          "extensive coverage",
          "automation process",
          "administrative task",
          "individual applicants",
          "promotion opportunities",
          "ib ili",
          "inclusion criteria",
          "potential, objective",
          "dominant language",
          "following section",
          "main characteristics",
          "Full-text articles",
          "non-English articles",
          "main findings",
          "806 Business Research",
          "research process",
          "common databases",
          "severe consequences",
          "societal decisions",
          "4 Descriptive results",
          "main focus",
          "Additional records",
          "perceived fairness",
          "individuals Records",
          "Yarger",
          "Hoffmann",
          "Sumser",
          "comment",
          "Varghese",
          "Horton",
          "Gil-Lafuente",
          "Oh",
          "3.4 Limitations",
          "approach",
          "reliance",
          "validity",
          "replicability",
          "Podsakoff",
          "pay",
          "situation",
          "crucial",
          "duplicates",
          "reasonsb",
          "Fig.",
          "aTopic",
          "English-language",
          "procedure",
          "practice",
          "Morrison",
          "name",
          "applied",
          "method",
          "trigger",
          "Figure",
          "distribution",
          "first",
          "sample",
          "Human Resource Management Review",
          "ar ch S",
          "ar ch N",
          "P S",
          "U S",
          "D N",
          "methodological perspective",
          "noteworthy result",
          "non-empirical evidence",
          "conceptual paper",
          "scientific investigation",
          "quantitative papers",
          "geographical specification",
          "information systems",
          "behavioral sciences",
          "high-ranked journals",
          "P F",
          "g K",
          "A C",
          "H R",
          "M B",
          "L ac",
          "ec ifi",
          "management research",
          "Management Science",
          "Management Annals",
          "enormous interest",
          "new topic",
          "Thirteen articles",
          "fourteen articles",
          "four articles",
          "808 Business Research",
          "case studies",
          "Twelve studies",
          "ss B",
          "large majority",
          "12 studies",
          "predominance",
          "reviews",
          "area",
          "USA",
          "psychology",
          "core",
          "Ethics",
          "Academy",
          "ex",
          "ro",
          "rv",
          "tl",
          "rd",
          "ly",
          "iz",
          "fr",
          "sk",
          "g H R",
          "ro fe ss",
          "U S A",
          "ss S",
          "A lg",
          "A K",
          "L aw",
          "810 Business Research",
          "ls B",
          "av ag",
          "D ev",
          "sy st",
          "ic",
          "ac",
          "rp",
          "ay",
          "ec",
          "cl",
          "sc",
          "rm",
          "ct",
          "tr",
          "iq",
          "rk",
          "eq",
          "ie",
          "ld",
          "cr",
          "iv",
          "rg",
          "ra",
          "ifi",
          "id"
        ]
      }
    ]
  }
/////////////////
4 - Query
{
    "search": "search=AI",
    "select": "metadata_author,metadata_title",
    "filter": "metadata_author ne 'Alina Köchling ' ",
    "facets": [ "metadata_author,count:5" ],
    "top": 5
}
{
    "@odata.context": "https://cognitive-search-example.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
    "@search.facets": {
      "metadata_author": [
        {
          "count": 1,
          "value": " Shun Kodate "
        },
        {
          "count": 1,
          "value": "Boping Zhang"
        },
        {
          "count": 1,
          "value": "Chunyong Yin "
        },
        {
          "count": 1,
          "value": "Ferdousi Sabera Rawnaque "
        },
        {
          "count": 1,
          "value": "Haoliang Cui"
        }
      ]
    },
    "value": [
      {
        "@search.score": 3.1117277,
        "metadata_author": "Tuba Parlar ",
        "metadata_title": "QER: a new feature selection method for sentiment analysis"
      },
      {
        "@search.score": 2.507882,
        "metadata_author": "Taiwo Kolajo ",
        "metadata_title": "Big data stream analysis: a systematic literature review"
      },
      {
        "@search.score": 2.316279,
        "metadata_author": "Partheeban Chandrasekaran",
        "metadata_title": "Toward a testbed for evaluating computational trust models: experiments and analysis"
      },
      {
        "@search.score": 2.2285721,
        "metadata_author": null,
        "metadata_title": null
      },
      {
        "@search.score": 1.8475893,
        "metadata_author": "Boping Zhang",
        "metadata_title": "Augmented reality virtual glasses try-on technology based on iOS platform"
      }
    ]
  }

/////////////////
5 - Query
{
    "search": "search=DevOps",
    "select": "level,role,instructor, duration, rating_average",
    "filter": "level ne 'advanced' ",
    "facets": [ "rating_average,values:3|4|5" ],
    "top": 5
}

{
    "@odata.context": "https://cognitive-search-example.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
    "@search.facets": {
      "rating_average": [
        {
          "count": 0,
          "to": 3
        },
        {
          "count": 1,
          "from": 3,
          "to": 4
        },
        {
          "count": 2,
          "from": 4,
          "to": 5
        },
        {
          "count": 0,
          "from": 5
        }
      ]
    },
    "value": [
      {
        "@search.score": 1.1356969,
        "duration": "3",
        "instructor": "Claudia Blackman",
        "level": "intermediate",
        "rating_average": 3.8,
        "role": "developer"
      },
      {
        "@search.score": 0.25811607,
        "duration": "82",
        "instructor": null,
        "level": "beginner",
        "rating_average": 4.73,
        "role": "solution-architect"
      },
      {
        "@search.score": 0.25316024,
        "duration": "5",
        "instructor": "Claudia Blackman",
        "level": "intermediate",
        "rating_average": 4.9,
        "role": "admin"
      }
    ]
  }